# -*- coding: utf-8 -*-
"""stock_price.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14cVwt8LtIgm6p3uRW3hWqqYFpkbhcRBS

# Installations/imports
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as scs
import statsmodels.api as sm
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.stattools import kpss
from scipy.stats import normaltest
from statsmodels.tsa.stattools import acf,pacf
#from pmdarima.arima import auto_arima
import scipy.interpolate as sci
import scipy.optimize as sco
from sklearn.metrics import mean_squared_error, mean_absolute_error
import math
import scipy.optimize as sco

"""Load CSV"""

df_org = pd.read_csv("stock_price_translated.csv")
df = df_org.copy()

"""# Data Understanding and EDA"""

df.head()

df.info()

"""The dataset contains 9202 entries and 7 columns:

Date: Stock data date (string).

Closing price: Closing price of the stock (float).

Opening price: Opening price of the stock (float).

High price: Highest price during the day (float).

Low price: Lowest price during the day (float).

Volume: Trading volume, currently formatted as strings (e.g., "79.15M").

Rate of change %: Daily percentage change in stock price, also formatted as strings (e.g., "-2.56%").
"""

# The summary statistics for numerical columns

df.describe()

# Check for missing values

missing_values = df.isnull().sum()
missing_values

"""There are no null(NaN) values in the dataset

it seems Volume column has values in both millions and billions
"""

billion_rows = df[df['Volume'].str.contains('B', na=False)]
print("Rows with volume in billions:\n", billion_rows)

"""Dropping high  values to get a better avrerage representation"""

# Drop rows where 'Volume' contains 'B'
df = df[~df['Volume'].str.contains('B', na=False)]
df.reset_index(drop=True, inplace=True)

# Adjust the 'Volume' conversion to handle both 'M' (millions) and 'B' (billions)
df['Volume'] = df['Volume'].replace({'M': 'e6'}, regex=True).astype(float)

# Convert 'Rate of change %' by removing '%' and converting to a numeric type
df['Rate of change %'] = df['Rate of change %'].replace({'%': ''}, regex=True).astype(float)

# Convert 'Date' column to datetime format
df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y')

# Check for summary statistics for numerical columns
summary_statistics = df.describe()
summary_statistics

# Plotting Opening and Closing Prices over time
plt.figure(figsize=(16,6))

# Plot Opening Prices
plt.plot(df['Date'], df['Opening price'], label='Opening Price', color='red')

# Plot Closing Prices
plt.plot(df['Date'], df['Closing price'], label='Closing Price', color='green')

# Add labels and title
plt.title('Opening vs Closing Price Over Time')
plt.xlabel('Date')
plt.ylabel('Price')
plt.legend()

# Show the plot
plt.show()

# Plotting Total Volume of Stocks Traded
plt.figure(figsize=(16,6))

plt.plot(df['Date'], df['Volume'], color='blue', label='Total Volume Traded')

# Add labels and title
plt.title('Total Volume of Stocks Traded Each Day')
plt.xlabel('Date')
plt.ylabel('Volume of Stocks')
plt.legend()

# Show the plot
plt.show()

# Plotting the Rate of Change %
plt.figure(figsize=(16,6))

plt.plot(df['Date'], df['Rate of change %'], color='orange', label='Rate of Change %')

# Add labels and title
plt.title('Rate of Change % Over Time')
plt.xlabel('Date')
plt.ylabel('Rate of Change %')
plt.legend()

plt.axhline(y=0, color='magenta', linestyle='--', label='0% Change')

# Show the plot
plt.show()

"""Removing rows where 'Rate of change %' is greater than 10% as it may creare bias to the average/ moving average values to be taken later."""

#  Function to removes rows where 'Rate of change %' is greater than 10%
def remove_high_rate_of_change(df):

    df['Rate of change %'] = df['Rate of change %'].replace({'%': ''}, regex=True).astype(float)
    cleaned_df = df[df['Rate of change %'] <= 10]

    return cleaned_df

df = remove_high_rate_of_change(df)
df.head()

"""Analysing Monthly Mean of Closing price"""

# Set 'Date' as the index
df.set_index('Date', inplace=True)

monthly_mean_close = df['Closing price'].resample('M').mean()

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(monthly_mean_close.index, monthly_mean_close, color='skyblue', marker='o', label='Monthly Mean Closing Price')

# Add labels and title
plt.title('Monthly Mean of Closing Price')
plt.xlabel('Month')
plt.ylabel('Mean Closing Price')
plt.legend()

# Show the plot
plt.grid()
plt.show()

monthly_mean_close.head(5)

"""Annual"""

annual_mean_close = df['Closing price'].resample('Y').mean()

# Plotting the Annual Mean Closing Price as a bar graph
plt.figure(figsize=(12, 6))
annual_mean_close.plot(kind='bar', color='skyblue')

# Add labels and title
plt.title('Annual Mean of Closing Price')
plt.xlabel('Year')
plt.ylabel('Mean Closing Price')
plt.xticks(rotation=45)

# Show the plot
plt.grid(axis='y')
plt.tight_layout()
plt.show()

"""Relationship between Volume and Closing Price"""

# Scatter plot to show the relationship between Volume and Closing Price
plt.figure(figsize=(10, 6))
plt.scatter(df['Volume'], df['Closing price'], alpha=0.5, color='blue')
plt.title('Relationship Between Volume and Closing Price')
plt.xlabel('Volume of Stocks Traded')
plt.ylabel('Closing Price')
plt.grid()

# Calculate the correlation coefficient
correlation = df['Volume'].corr(df['Closing price'])
plt.text(0.1, 0.9, f'Correlation: {correlation:.2f}', transform=plt.gca().transAxes)

# Show the plot
plt.show()

"""**Moving Averages** (Technical Indicator)"""

# Moving averages (5-day and 20-day moving averages)

'''
df['MA_5'] = df['Closing price'].rolling(window=5).mean()
df['MA_20'] = df['Closing price'].rolling(window=20).mean()
 '''

"""**Volatility Features**"""

# Volatility features (7-day rolling standard deviation)
'''
df['Volatility_7'] = df['Closing price'].rolling(window=7).std()
'''

"""# 2. ARIMA

**Checking close price data for normality and stationarity**

* ADF test
* Normality Test

ADF test
"""

def adf_test(df):
  result = adfuller(df, autolag='AIC')
  print('ADF Statistic: %f' % result[0])
  print('p-value: %f' % result[1])
  print('Critical Values:')
  for key, value in result[4].items():
      print('\t%s: %.3f' % (key, value))
  if result[0] < result[4]["5%"]:
      print ("Reject Null Hypothesis. So, Time Series is Stationary")
  else:
      print ("Failed to reject Null Hypothesis. So, Time Series is Not-Stationary")
  print("\n")

adf_test(df['Closing price'])

"""Normality Test"""

def normality_test(df):
  result = adfuller(df, autolag='AIC')
  stat, p = normaltest(df)
  print('Statistics=%.3f, p=%.3f' % (stat, p))
  print('Critical Values:')
  for key, value in result[4].items():
      print('\t%s: %.3f' % (key, value))
  alpha = 0.05
  if p > alpha:
      print('Data is normally distributed (fail to reject H0)')
  else:
      print('Data is not normally distributed(reject H0)')
  print('\n')

normality_test(df['Closing price'])

"""## Log transformation

The log transformation can be used to make highly skewed distributions less skewed.
"""

df_log = np.log(df / df.shift(1))
df_log.head()

try:
    df_log.hist(bins=50, figsize=(10, 8))
    plt.show()
except ValueError as e:
    print(f"Warning: {e}. Ignoring this error and continuing...")

df_log2 = df_log.dropna()

adf_test(df_log2['Closing price'])

normality_test(df_log2['Closing price'])

"""QQ plot"""

fig = sm.qqplot(df_log2['Closing price'], line='s')
plt.show()

"""## Constructing an ARIMA model with auto-fitting parameters (p,d,q)"""

!pip install pmdarima

from pmdarima import auto_arima

target=['Closing price','High price', 'Low price']

def arima_auto(arr):
    train_data=pd.DataFrame() # splitting into test and train data
    test_data=pd.DataFrame()
    plt.figure(figsize=(12,8))
    for i, sym in enumerate(target):
        train_data[sym], test_data[sym] = df[sym][1:int(len(df[sym])*0.8)], df[sym][int(len(df)*0.2):]
        model = auto_arima(train_data[sym], trace=True, error_action='ignore', suppress_warnings=True)
        model.fit(train_data[sym]) # fitting the model
        forecast = model.predict(n_periods=len(test_data[sym]))
        forecast = pd.DataFrame(forecast,index = test_data.index) # plot the predictions for validation set
        plt.plot(train_data, label='Train {}'.format(sym))
        plt.plot(test_data, label='Test {}'.format(sym))
        plt.plot(forecast, label='Prediction {}'.format(sym))
    plt.title('Stocks Price Prediction\n')
    plt.xlabel('Date')
    plt.ylabel('Actual Stock Price')
    handles, labels = plt.gca().get_legend_handles_labels()
    by_label = dict(zip(labels, handles))
    plt.legend(by_label.values(), by_label.keys())
#    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)
    plt.show()

arima_auto(df)

"""Best fit ARIMA (p,d,q) parameters:

Closing price : (2,1,3)  
High price : (2,1,2)  
Low price : (3,1,3)
"""

import pickle

# Save the trained ARIMA model using the pickle module
with open('arima_model_pickle.pkl', 'wb') as file:
    pickle.dump(arima_auto, file)

'''
# Load the saved ARIMA model using statsmodels load method
from statsmodels.tsa.arima.model import ARIMAResults
loaded_model_arima = ARIMAResults.load('arima_model.pkl')

# load the saved ARIMA model using the pickle module
with open('arima_model_pickle.pkl', 'rb') as file:
    loaded_model_arima = pickle.load(file)

predictions = loaded_model_arima.forecast(steps=30)
'''









"""# LSTM"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError, MeanAbsoluteError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam
from tensorflow.keras import regularizers
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

"""### Data Preprocessing"""

# Selecting the 'Closing price' for prediction
data = df['Closing price'].values.reshape(-1, 1).copy()

# Scaling the data using MinMaxScaler to bring values between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)

# Define a function to create sequences of data (this is required for LSTM)
def create_sequences(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i + time_steps, 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)

# Define the number of time steps (how many previous days the model looks at)
time_steps = 90

# Create sequences
X, y = create_sequences(scaled_data, time_steps)

# Reshape X for LSTM [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

# Split the data into training and testing sets (80% train, 20% test)
train_size = int(X.shape[0] * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

"""### Building the LSTM Model"""

# Initialize the LSTM model
model = Sequential()

# Adding the first LSTM layer with 50 units, and return sequences True since we have more LSTM layers
model.add(LSTM(units=50, return_sequences=True, input_shape=(time_steps, 1)))
model.add(Dropout(0.2))  # Adding dropout to avoid overfitting

# Adding the second LSTM layer with 50 units
model.add(LSTM(units=50, return_sequences=False))
model.add(Dropout(0.2))

# Adding the output layer
model.add(Dense(units=1))  # Output layer for predicting the next closing price

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

model.summary()

history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))

# Predicting on the test data
y_pred_scaled = model.predict(X_test)

# Inversely transform the scaled predictions to get the actual price values
y_pred = scaler.inverse_transform(y_pred_scaled.reshape(-1, 1))
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# 5. Plotting the Predictions vs Actual Data
plt.figure(figsize=(10, 6))
plt.plot(df.index[-len(y_test_actual):], y_test_actual, color='blue', label='Actual Closing Price')
plt.plot(df.index[-len(y_pred):], y_pred, color='red', label='Predicted Closing Price')
plt.title('LSTM Model - Stock Price Prediction')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.legend()
plt.show()

# saving LSTM model

# Method 1: Save as HDF5 format
model.save('lstm_model.h5')  # Saves the entire model (architecture, weights, optimizer)
'''
# Method 2: Save in TensorFlow's SavedModel format
model.save('lstm_model')  # Creates a directory with model info
'''

'''
# loading LSTM model
import tensorflow as tf
# Method 1: Load from HDF5 format
loaded_model = tensorflow.keras.models.load_model('lstm_model.h5')

# Method 2: Load from SavedModel format
loaded_model = tensorflow.keras.models.load_model('lstm_model')
'''





"""# Prophet"""

# Create the 'y' (target variable for Prophet)
# Create a 'ds' column from the existing index (which is Date)
df_pft = df.copy()
df_pft["ds"] = df_pft.index
df_pft["y"] = df_pft['Closing price']

# Drop any NaN values (if necessary)
df_pft.dropna(subset=['y'], inplace=True)

from prophet import Prophet

# Initialize the Prophet model
model_fbp = Prophet()

# Fit the Prophet model to the data
model_fbp.fit(df_pft[['ds', 'y']])

# Make predictions
future = model_fbp.make_future_dataframe(periods=30)  # Forecast 30 days into the future
forecast = model_fbp.predict(future)

# Plot the forecasted data
fig = model_fbp.plot(forecast)
plt.title('Prophet Model Forecast for Closing Prices')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.show()

# Plot forecast components (trend, seasonality, etc.)
fig2 = model_fbp.plot_components(forecast)
plt.show()

# Adding the forecasted values to the original dataframe for comparison
df_pft['Forecast_Prophet'] = forecast['yhat'][:len(df_pft)]

# Plot actual vs predicted values
plt.figure(figsize=(14, 7))
plt.plot(df_pft['ds'], df_pft['y'], label='Actual Closing Price', color='blue')
plt.plot(df_pft['ds'], df_pft['Forecast_Prophet'], label='Forecasted Closing Price (Prophet)', color='red')
plt.title('Actual vs Forecasted Closing Prices using Prophet')
plt.xlabel('Date')
plt.ylabel('Closing Price')
plt.legend()
plt.grid(True)
plt.show()

import pickle

# Save the trained model to a file
with open('prophet_model.pkl', 'wb') as file:
    pickle.dump(model_fbp, file)

"""
# Load the trained model from a file
with open('prophet_model.pkl', 'rb') as file:
    loaded_model_fbp = pickle.load(file)

# Now you can use the loaded model to make predictions
future = loaded_model_fbp.make_future_dataframe(periods=30)
forecast = loaded_model_fbp.predict(future)
"""









"""# Test performance"""

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

split_date = pd.to_datetime('2022-12-27')
df_train = df.loc[df.index <= split_date].copy()
df_test = df.loc[df.index > split_date].copy()

"""### ARIMA"""

from statsmodels.tsa.arima.model import ARIMA
import numpy as np

train = df_train['Closing price']
test = df_test['Closing price']

# Fit the ARIMA model (replace order=(p, d, q) with your ARIMA parameters)
model_arima = ARIMA(train, order=(2, 1, 3))  # Specify your ARIMA (p, d, q) order
model_arima_fit = model_arima.fit()

# Make predictions on the test set
start = len(train)  # Start predicting from the point where training data ends
end = start + len(test) - 1  # End prediction at the end of the test data
predictions = model_arima_fit.predict(start=start, end=end, dynamic=False)  # ARIMA predictions

mae = mean_absolute_error(test, predictions)
mse = mean_squared_error(test, predictions)
rmse = np.sqrt(mse)
r2 = r2_score(test, predictions)

# Print evaluation metrics
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'R-squared (R²): {r2}')



"""### LSTM"""

# Selecting the 'Closing price' for prediction
data = df_train['Closing price'].values.reshape(-1, 1).copy()
# Scaling the data using MinMaxScaler to bring values between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
# Define a function to create sequences of data (this is required for LSTM)
def create_sequences(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i + time_steps, 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)
time_steps = 90
# Create sequences
X, y = create_sequences(scaled_data, time_steps)

# Reshape X for LSTM [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

X_train = X[:]
y_train = y[:]

# Check the shapes of the reshaped arrays
print(f'Shape of X_train: {X_train.shape}')
print(f'Shape of y_train: {y_train.shape}')

# Selecting the 'Closing price' for prediction
data = df_test['Closing price'].values.reshape(-1, 1).copy()
# Scaling the data using MinMaxScaler to bring values between 0 and 1
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data)
# Define a function to create sequences of data (this is required for LSTM)
def create_sequences(data, time_steps):
    X, y = [], []
    for i in range(len(data) - time_steps):
        X.append(data[i:i + time_steps, 0])
        y.append(data[i + time_steps, 0])
    return np.array(X), np.array(y)
time_steps = 90
# Create sequences
X, y = create_sequences(scaled_data, time_steps)

# Reshape X for LSTM [samples, time steps, features]
X = np.reshape(X, (X.shape[0], X.shape[1], 1))

X_test = X[:]
y_test = y[:]

# Check the shapes of the reshaped arrays
print(f'Shape of X_train: {X_test.shape}')
print(f'Shape of y_train: {y_test.shape}')

# loading LSTM model
import tensorflow as tf
loaded_model_lstm = tf.keras.models.load_model('lstm_model.h5')
loaded_model_lstm.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])

y_pred = loaded_model_lstm.predict(X_test)  # Replace with actual LSTM prediction

# Calculate evaluation metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

# Print the results
print(f'MAE: {mae}')
print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'R-squared (R²): {r2}')





"""### PROPHET"""

# Prepare your test dataset

df_test = pd.DataFrame({
    'ds': pd.to_datetime(df_test.index),
    'y': df_test['Closing price']
})

# Load the trained model from a file
with open('prophet_model.pkl', 'rb') as file:
    loaded_model_fbp = pickle.load(file)

# Now you can use the loaded model to make predictions
future = loaded_model_fbp.make_future_dataframe(periods=30)
forecast = loaded_model_fbp.predict(future)

# Get the predictions for the test dates
predictions = forecast[forecast['ds'].isin(df_test['ds'])]['yhat'].values

# Evaluate the model's performance
mae = mean_absolute_error(df_test['y'], predictions)
mse = mean_squared_error(df_test['y'], predictions)
rmse = mean_squared_error(df_test['y'], predictions, squared=False)  # RMSE
r2 = r2_score(df_test['y'], predictions)

# Print evaluation metrics
print(f'Mean Absolute Error (MAE): {mae}')
print(f'Mean Squared Error (MSE): {mse}')
print(f'Root Mean Squared Error (RMSE): {rmse}')
print(f'R-squared (R²): {r2}')







"""Consideration of Improvements and Model Retraining

Overview:

* The performance metrics indicate that the LSTM model outperforms both the ARIMA and Prophet models, particularly in terms of R² and error metrics. The ARIMA and Prophet models have negative R² values, suggesting poor fit compared to a simple mean model.

**Areas for Improvement:**


Data Quality:
* Ensure that the data is clean and free of outliers, missing values, and
non-finite values.
Feature Engineering: Experiment with additional features or transformations that may enhance model performance (e.g., adding lag features, moving averages, or external regressors).

* Hyperparameter Tuning: Optimize hyperparameters for the LSTM model using techniques like grid search or randomized search.
* Regularization: Introduce regularization techniques in LSTM to avoid overfitting.
* Cross-Validation: Implement cross-validation techniques to assess model robustness across different subsets of the data.

Summarizing Results and Creating Presentation Materials

Overview:

Model Comparison:

ARIMA:
* MAE: 10.05
* MSE: 145.63
* RMSE: 12.07
* R²: -0.34

The ARIMA model demonstrates relatively high error metrics and a negative R² value, indicating that it does not fit the data better than a simple mean model. This suggests that the ARIMA approach may not capture the underlying trends or seasonality effectively for this dataset.

LSTM:
* MAE: 0.036
* MSE: 0.0022
* RMSE: 0.047
* R²: 0.951

The LSTM model significantly outperforms the other models, as evidenced by its low error metrics and high R² value. This indicates that the LSTM captures the complexities of the time series data, providing accurate predictions with minimal error.

Prophet:
* MAE: 13.64
* MSE: 268.31
* RMSE: 16.38
* R²: -1.47

Similar to ARIMA, the Prophet model exhibits high error metrics and a negative R² value, suggesting inadequate performance. While Prophet is designed for handling seasonality and trends, its results in this case indicate that it may not be suitable for the specific characteristics of the stock price dataset.


In summary, while the LSTM model currently provides the most accurate predictions for the stock price dataset, there is room for improvement across all models. By enhancing data quality, employing robust feature engineering, optimizing model parameters, and experimenting with ensemble methods, we can work towards developing a more accurate and reliable prediction model. These efforts not only enhance performance but also contribute to a deeper understanding of the underlying data dynamics.
"""

